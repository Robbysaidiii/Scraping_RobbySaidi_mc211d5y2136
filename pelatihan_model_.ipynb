{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDCi1MMNSTiWBBBythcTqa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Robbysaidiii/Scraping_RobbySaidi_mc211d5y2136/blob/main/pelatihan_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libarary yang dibutuhkn"
      ],
      "metadata": {
        "id": "EwD3_0W7foY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install tensorflow\n",
        "!pip install swifter\n",
        "!pip install transformers\n",
        "!pip install textblob\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqbyy_Sk7quC",
        "outputId": "de47e4f4-d5b8-475f-ca9a-b35f1af84182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import swifter\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
      ],
      "metadata": {
        "id": "oluz34Drekbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "UJvtEAx07nMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memuat Dataset"
      ],
      "metadata": {
        "id": "-hMzC56Gf4Xo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6b62qnk_804"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df=pd.read_csv('/content/drive/MyDrive/ulasan_aplikasi_twitter.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "3AkwfSJQgnWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "k-Walfx7hQmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari dataframe diatas terdapat username, rating,review , tanggal\n",
        "\n",
        "1.  (username) hanya berisi Pengguna Google yang artinya google menyembunyikan nama penggunna\n",
        "\n",
        "2.   (Rating) memberika bintang sesuai dengan permasalahan user\n",
        "\n",
        "3.  (Review) sebuah ekspersi user saat menggunkan aplikasi x/Twitter\n",
        "\n",
        "4.  (Tanggal) setiap user yag memberika review dan rting akan tercatat tanggalnya\n",
        "\n"
      ],
      "metadata": {
        "id": "dgK3f0Iahvza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0HOB4OlzhWEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari dataframe diatas terdapat 11110 data yang berisi 4 kolom dengan tipe data 3 object dan 1 int64.... ridak ada yang missing value"
      ],
      "metadata": {
        "id": "ekd73YyjmE6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Rating'].value_counts()\n",
        "df['userName'].nunique()"
      ],
      "metadata": {
        "id": "a6KtxC8Nve-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "SmCO6rLU8r8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Review.duplicated().sum()\n",
        "df.userName.duplicated().sum()"
      ],
      "metadata": {
        "id": "r5b3siTh8z9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "karna di dalam username semua nya sama jadi terkena duplikat tapi saya tidak hapus duplikat karna sangat penting untuk analisis"
      ],
      "metadata": {
        "id": "X6n4MCCN96K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "57rtnrRcm_5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. userName\n",
        "Jumlah total: 11.111\n",
        "\n",
        "Unik: 1\n",
        "\n",
        "Nilai paling umum: \"Pengguna Google\" (muncul di semua baris)\n",
        "\n",
        "Insight:\n",
        "\n",
        "Semua nama pengguna disamarkan.\n",
        "\n",
        "Kolom userName tidak informatif untuk analisis, bisa diabaikan atau dibuang.\n",
        "\n",
        "2. Rating\n",
        "Tipe data: Numerik\n",
        "\n",
        "Statistik:\n",
        "\n",
        "Rata-rata (mean): 1.99\n",
        "\n",
        "Standar deviasi: 1.43\n",
        "\n",
        "Minimum: 1\n",
        "\n",
        "Kuartil 25%: 1\n",
        "\n",
        "Median (50%): 1\n",
        "\n",
        "Kuartil 75%: 3\n",
        "\n",
        "Maksimum: 5\n",
        "\n",
        "Insight:\n",
        "\n",
        "Distribusi rating berat ke arah negatif.\n",
        "\n",
        "Median = 1 ‚Üí lebih dari setengah user memberi bintang 1.\n",
        "\n",
        "Hanya sebagian kecil yang memberi bintang 4‚Äì5.\n",
        "\n",
        "3. Review\n",
        "Jumlah total review: 11.111\n",
        "\n",
        "Jumlah unik: 11.101\n",
        "\n",
        "Review paling sering muncul: \"Sangat membantu\" (muncul 3 kali)\n",
        "\n",
        "Insight:\n",
        "\n",
        "Hampir semua review berbeda ‚Üí sangat beragam.\n",
        "\n",
        "Duplikasi sangat kecil (hanya 10 duplikat dari 11.111 data), bisa diabaikan atau dibersihkan jika perlu.\n",
        "\n",
        "4. Tanggal\n",
        "Jumlah tanggal unik: 11.107\n",
        "\n",
        "Tanggal paling sering muncul: 2025-03-11 10:43:31 (muncul 2 kali)\n",
        "\n",
        "Insight:\n",
        "\n",
        "Data dikumpulkan dalam waktu yang cukup variatif.\n",
        "\n",
        "Distribusi waktu cukup baik, tidak terfokus di satu hari.\n",
        "\n",
        " Kesimpulan EDA (Dataset 11.111 Review):\n",
        "Sentimen pengguna cenderung negatif, mayoritas rating bintang 1.\n",
        "\n",
        "Review sangat beragam dan organik.\n",
        "\n",
        "Data waktu dan review cukup menyebar ‚Üí bagus untuk analisis lanjutan seperti time-series atau tren.\n",
        "\n",
        "Kolom userName bisa diabaikan karena tidak memberikan informasi tambahan.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HJpQOhPozaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Rating'], bins=5, kde=True, color='skyblue')\n",
        "plt.title('Distribusi Rating Pengguna')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Jumlah Ulasan')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EGDpUs14nAPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mayoritas rating adalah 1 bintang ‚Äî Ini menandakan banyak pengguna tidak puas dengan aplikasi (kemungkinan besar review-nya negatif).\n",
        "\n",
        "Sedikit yang kasih 2‚Äì3 bintang ‚Äî Artinya ulasan netral atau setengah-setengah itu jarang.\n",
        "\n",
        "Rating tinggi (4‚Äì5) juga cukup signifikan ‚Äî Menunjukkan ada juga yang puas atau sangat puas."
      ],
      "metadata": {
        "id": "jqMa9SN-sc3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pastikan kolom Tanggal dalam format datetime\n",
        "df['Tanggal'] = pd.to_datetime(df['Tanggal'], errors='coerce')\n",
        "\n",
        "# Group by tahun dan rating\n",
        "rating_per_tahun = df.groupby([df['Tanggal'].dt.year, 'Rating']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "rating_per_tahun.plot(kind='bar', stacked=True, colormap='Set3', figsize=(10, 6))\n",
        "\n",
        "plt.title(\"Distribusi Rating Ulasan per Tahun\")\n",
        "plt.xlabel(\"Tahun\")\n",
        "plt.ylabel(\"Jumlah Ulasan\")\n",
        "plt.legend(title=\"Rating\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XiWtJVXpq7R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahun 2020 & 2023 jadi tahun paling banyak ulasan.\n",
        "\n",
        "Rating 1 (warna biru) mendominasi setiap tahun ‚Üí ini menandakan banyak ulasan negatif dari pengguna.\n",
        "\n",
        "Rating 5 (kuning) juga cukup signifikan di beberapa tahun, misalnya 2020 dan 2025.\n",
        "\n",
        "Tahun 2018 masih sedikit aktivitas (wajar, mungkin awal-awal).\n",
        "\n",
        "Tren dari 2019‚Äì2024 cukup stabil, tapi ulasan positif vs negatif masih jauh berbeda."
      ],
      "metadata": {
        "id": "UL9VHKVl4dYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EDA: Distribusi Panjang Teks ---\n",
        "df['review_length'] = df['Review'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(df['review_length'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribusi Panjang Review')\n",
        "plt.xlabel('Jumlah Kata')\n",
        "plt.ylabel('Jumlah Review')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g3SjCm3PT2IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari hasil visualisasi panjang pesan review user yang paling banayak ada di 25 untuk panjang kata 60-100 hanya sedikit"
      ],
      "metadata": {
        "id": "Ip8HP3qZLC5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan semua teks review\n",
        "text_all = ' '.join(df['Review'].astype(str))\n",
        "\n",
        "# Buat Word Cloud\n",
        "wordcloud_all = WordCloud(width=1000, height=500, background_color='white', colormap='viridis').generate(text_all)\n",
        "\n",
        "# Tampilkan Word Cloud\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud - Semua Review\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MidCg0kcRRZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dari hasil wordcloud semua analisissentimen paling banyak ada keluhan atau negatif"
      ],
      "metadata": {
        "id": "Y5kDBQ29LUwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Pra-pemrosesan Teks"
      ],
      "metadata": {
        "id": "wpDnt_Cq53Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Case Folding\n",
        "df['Review'] = df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "zuDGmd2Z58nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mengubah semua huruf jadi huruf keci"
      ],
      "metadata": {
        "id": "dZgZv2e06F_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Special Characters, Angka, dan Punctuation\n",
        "df['Review'] = df['Review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))"
      ],
      "metadata": {
        "id": "Sz_IT5X06PzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buang simbol, angka, dan tanda baca yang nggak penting."
      ],
      "metadata": {
        "id": "qT43nMw16mq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing\n",
        "df['tokens'] = df['Review'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "kjCsTLKn6oGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "memecah kalimat menjadi potongan-potongan kata"
      ],
      "metadata": {
        "id": "sn3omukgCQcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
      ],
      "metadata": {
        "id": "iPqaRdb57EGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Menghapus stopwords, yaitu kata-kata umum yang sering muncul tapi nggak punya makna penting dalam analisis"
      ],
      "metadata": {
        "id": "7kaVlsDVCbwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil semua kata dari seluruh token\n",
        "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
        "unique_words = list(set(all_words))\n",
        "\n",
        "# Inisialisasi stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Fungsi bantu untuk stemming\n",
        "def stem_word(word):\n",
        "    return word, stemmer.stem(word)\n",
        "\n",
        "# Stemming paralel dengan joblib\n",
        "stemmed_list = Parallel(n_jobs=-1)(delayed(stem_word)(word) for word in tqdm(unique_words))\n",
        "stemmed_dict = dict(stemmed_list)\n",
        "\n",
        "# Replace semua token pakai dictionary\n",
        "df['tokens'] = df['tokens'].swifter.apply(lambda x: [stemmed_dict[word] for word in x])"
      ],
      "metadata": {
        "id": "Vl-DEhSG7Naz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bertujuan untuk melakukan proses stemming secara efisien pada data token dalam Bahasa Indonesia. Pertama, semua kata dari token di setiap review dikumpulkan menjadi satu list besar (all_words), kemudian diubah menjadi kumpulan kata unik (unique_words) agar tidak melakukan stemming berulang-ulang untuk kata yang sama. Selanjutnya, dibuat kamus stemming (stemmed_dict) dengan menggunakan Sastrawi Stemmer yang memetakan setiap kata unik ke bentuk dasarnya. Terakhir, setiap token dalam kolom df['tokens'] diganti dengan hasil stemming yang diambil dari kamus tersebut. Teknik ini membantu mempercepat proses stemming dan menjaga konsistensi hasil stemming di seluruh dataset."
      ],
      "metadata": {
        "id": "pVFeBPXPUDXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ekstraksi fitur dan pelabelan data"
      ],
      "metadata": {
        "id": "n-xXp3plPlgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi ini menghasilkan teks yang lebih bersih dan representatif untuk analisis lebih lanjut seperti klasifikasi atau ekstraksi fitur."
      ],
      "metadata": {
        "id": "0gNNs5bpUcSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi hanya sekali\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Fungsi hanya fokus mengembalikan skor\n",
        "def get_vader_sentiment(text):\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "# Terapkan ke kolom\n",
        "df['vader_compound'] = df['cleaned_review'].apply(get_vader_sentiment)\n",
        "\n",
        "# Label berdasarkan nilai polaritas TextBlob\n",
        "def label_textblob(polarity):\n",
        "    if polarity > 0:\n",
        "        return 'positif'\n",
        "    elif polarity < 0:\n",
        "        return 'negatif'\n",
        "    else:\n",
        "        return 'netral'\n",
        "\n",
        "df['textblob_label'] = df['textblob_polarity'].apply(label_textblob)\n",
        "\n",
        "# Label berdasarkan nilai compound VADER\n",
        "def label_vader(score):\n",
        "    if score >= 0.05:\n",
        "        return 'positif'\n",
        "    elif score <= -0.05:\n",
        "        return 'negatif'\n",
        "    else:\n",
        "        return 'netral'\n",
        "\n",
        "df['vader_label'] = df['vader_compound'].apply(label_vader)\n"
      ],
      "metadata": {
        "id": "GD3mXE0jQNdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Gabungkan token hasil preprocessing\n",
        "df['cleaned_review'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# 2. Analisis sentimen\n",
        "df['textblob_polarity'] = df['cleaned_review'].apply(get_textblob_polarity)\n",
        "df['vader_compound'] = df['cleaned_review'].apply(get_vader_sentiment)\n"
      ],
      "metadata": {
        "id": "DucfmlZoBrvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['Review', 'cleaned_review', 'textblob_polarity', 'textblob_label', 'vader_compound', 'vader_label']].head(50)\n"
      ],
      "metadata": {
        "id": "0RmX7oy7D0EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mengaktifkan fitur progress bar pada pandas.apply() sehingga saat fungsi preprocess_text diterapkan ke setiap review (df['Review']). Proses ini cukup berat karena melakukan normalisasi teks, tokenisasi, stopword removal, dan stemming untuk 11.111 baris data, yang terlihat memakan waktu sekitar 2,5 jam. Setelah teks dibersihkan dan disimpan dalam kolom cleaned_review, data diberi label sentimen: negatif (rating ‚â§ 2), netral (rating = 3), dan positif (rating ‚â• 4). Label ini kemudian diencoding secara numerik dengan LabelEncoder agar bisa digunakan oleh model klasifikasi. Terakhir, hasil distribusi label ditampilkan, menunjukkan bahwa mayoritas review bersentimen negatif, sesuai dengan rata-rata rating rendah yang sebelumnya ditemukan di EDA.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  Dataset memiliki tiga kelas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5bEbtecYVGiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modeling"
      ],
      "metadata": {
        "id": "5e07uEBBRbtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  SKEMA PELATIHAN 1: LSTM + EMBEDDING\n",
        "\n",
        "\n",
        "\n",
        "*   Menggunakan algoritma deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "zLgYwX3ekpqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TargetAccuracyCallback(Callback):\n",
        "    def __init__(self, target_acc=0.92):\n",
        "        super().__init__()\n",
        "        self.target_acc = target_acc\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_acc = logs.get('val_accuracy')\n",
        "        if val_acc is not None:\n",
        "            if val_acc >= self.target_acc:\n",
        "                print(f\"\\nüéâ Target val_accuracy {self.target_acc*100:.2f}% tercapai di epoch {epoch+1}, menghentikan training.\")\n",
        "                self.model.stop_training = True\n"
      ],
      "metadata": {
        "id": "RnkdN-XhVuRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(df['cleaned_review'])\n",
        "X_seq = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
        "X_seq = pad_sequences(X_seq, maxlen=100)\n"
      ],
      "metadata": {
        "id": "w8A9lrVXqmHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
        "    X_seq, df['encoded_label'], test_size=0.2, stratify=df['encoded_label'], random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "5iD6tqUZQ5tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model LSTM\n",
        "model_lstm = Sequential([\n",
        "    Embedding(20000, 128, input_length=100),\n",
        "    LSTM(128, dropout=0.7, recurrent_dropout=0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_lstm = model_lstm.fit(\n",
        "    X_train1, y_train1,\n",
        "    validation_data=(X_test1, y_test1),\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint('best_lstm.h5', monitor='val_accuracy', save_best_only=True),\n",
        "        TargetAccuracyCallback(target_acc=0.95)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "uQUDdVBBRjYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikut adalah penjelasan per bagian dari proses training model kamu berdasarkan grafik dan hasil epoch:\n",
        "\n",
        "---\n",
        "\n",
        "###  **Initial Performance (Epoch 1)**\n",
        "\n",
        "- **Training Accuracy: 70.85%**, **Validation Accuracy: 77.37%**\n",
        "- Ini menunjukkan bahwa model sudah cukup baik di awal, bahkan **val accuracy lebih tinggi daripada train**, yang cukup tidak biasa.\n",
        "- Kemungkinan penyebab:\n",
        "  - **Model arsitekturnya cocok** dengan pola dalam data.\n",
        "  - **Inisialisasi bobot awal (initial weights)** cukup kuat.\n",
        "  - Tapi perlu waspada, karena val acc > train acc bisa juga disebabkan oleh **data leakage** (misalnya data testing yang terlalu mirip dengan training).\n",
        "\n",
        "---\n",
        "\n",
        "###  **Rapid Improvement Phase (Epoch 2‚Äì3)**\n",
        "\n",
        "- Train acc naik dari **79.15% ‚ûù 81.62%**, artinya model **belajar dengan cepat**.\n",
        "- Val acc juga meningkat (meskipun lebih pelan): **78.45% ‚ûù 79.49%**\n",
        "- Ini fase **pembelajaran aktif**, di mana model masih generalisasi dengan baik.\n",
        "- **Gap antara train dan val tidak terlalu besar**, artinya belum ada overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Peak Performance (Epoch 4)**\n",
        "\n",
        "- **Train Accuracy: 82.68%**, **Val Accuracy: 79.26%**\n",
        "- **Train Loss: 0.4825**, **Val Loss: 0.6093**\n",
        "- Ini adalah titik **performansi paling seimbang**, karena model:\n",
        "  - Memiliki akurasi tinggi pada kedua set.\n",
        "  - Loss pada val dan train cukup dekat dan menunjukkan **konvergensi**.\n",
        "- Bisa dianggap **epoch terbaik sebelum overfitting muncul**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Emerging Overfitting (Epochs 5‚Äì6)**\n",
        "\n",
        "- **Train Accuracy meningkat terus** (84.61% ‚ûù 86.24%)\n",
        "- Tapi **Validation Accuracy stagnan bahkan menurun** (78.36% ‚ûù 78.00%)\n",
        "- **Train Loss menurun**, tapi **Val Loss naik** ‚Üí tanda klasik **overfitting**\n",
        "- Artinya: model terlalu fokus ke data training dan mulai **gagal generalisasi ke data baru**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Kesimpulan Insight**\n",
        "\n",
        "- Model menunjukkan potensi sangat baik dan cocok untuk task ini.\n",
        "- Namun, performa terbaik hanya sampai **epoch 3‚Äì4**, setelah itu mulai overfit.\n",
        "- Solusi:\n",
        "  - Terapkan **early stopping** di sekitar epoch ke-3.\n",
        "  - Tambahkan **Dropout/L2 Regularization**.\n",
        "  - Atau pertimbangkan fine-tuning **pretrained model** seperti IndoBERT untuk performa lebih stabil.\n",
        "\n",
        "Kalau kamu mau, aku bisa bantu hitung skor evaluasi lebih lanjut (precision, recall, F1) atau buatkan confusion matrix-nya juga. Mau lanjut ke situ?"
      ],
      "metadata": {
        "id": "jMOdrHbFiO6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_lstm.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
        "plt.plot(history_lstm.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U1AytNAIZhum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight dari Grafik (Accuracy & Loss per Epoch)\n",
        "\n",
        "1. Akurasi Training meningkat stabil dari epoch 0 hingga 5 (0.73 ‚ûù 0.86), menandakan model belajar dari data dengan baik.\n",
        "\n",
        "2. Val Accuracy stagnan & mulai menurun sejak epoch ke-3, dari 0.79 ‚ûù 0.78. Ini indikasi awal overfitting.\n",
        "\n",
        "3. Train Loss terus turun, tapi Val Loss mulai naik setelah epoch ke-2. Ini semakin memperkuat dugaan bahwa model overfitting setelah titik tersebut.\n",
        "\n",
        "4. Epoch optimal kemungkinan besar ada di epoch ke-2 atau ke-3, karena di titik itu model masih seimbang antara belajar dan generalisasi.\n",
        "\n"
      ],
      "metadata": {
        "id": "ORrRWW8uhUrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  SKEMA PELATIHAN 2: CNN + TF-IDF\n",
        "\n",
        "\n",
        "\n",
        "*   Menggunakan algoritma deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "hDvEgAZwk8oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=20000)\n",
        "X_tfidf = tfidf.fit_transform(df['cleaned_review']).toarray()\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
        "    X_tfidf, df['encoded_label'], test_size=0.2, stratify=df['encoded_label'], random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "N8fPLjxquasT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=20000)\n",
        "X_tfidf = tfidf.fit_transform(df['cleaned_review']).toarray()\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
        "    X_tfidf, df['encoded_label'], test_size=0.2, stratify=df['encoded_label'], random_state=42\n",
        ")\n",
        "\n",
        "X_train2 = X_train2.reshape(X_train2.shape[0], X_tfidf.shape[1], 1)\n",
        "X_test2 = X_test2.reshape(X_test2.shape[0], X_tfidf.shape[1], 1)\n",
        "\n",
        "model_cnn = Sequential([\n",
        "    Conv1D(128, 5, activation='relu', input_shape=(X_tfidf.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization,\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_cnn = model_cnn.fit(\n",
        "    X_train2, y_train2,\n",
        "    validation_data=(X_test2, y_test2),\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
        "        ModelCheckpoint('best_cnn.h5', monitor='val_accuracy', save_best_only=True),\n",
        "         TargetAccuracyCallback(target_acc=0.95)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "88PtfvHclCpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['loss'], label='Train Loss')\n",
        "plt.plot(history_cnn.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MuPINOqPdvw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SKEMA PELATIHAN 3: INDOBERT\n",
        "\n",
        "\n",
        "\n",
        "*   Menggunakan algoritma deep learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "kM_koy14lUWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "X_bert = bert_tokenizer(\n",
        "    df['cleaned_review'].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=100,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
        "    X_bert['input_ids'],\n",
        "    to_categorical(df['encoded_label']),\n",
        "    test_size=0.2,\n",
        "    stratify=df['encoded_label'],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "FFn0NbAjuqb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_bert = TFBertForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p1',\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "model_bert.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history_bert = model_bert.fit(\n",
        "    X_train3, y_train3,\n",
        "    validation_data=(X_test3, y_test3),\n",
        "    epochs=3,\n",
        "    batch_size=16,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True),\n",
        "        ModelCheckpoint('best_bert', monitor='val_accuracy', save_best_only=True),\n",
        "         TargetAccuracyCallback(target_acc=0.95)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "bSlzTavylYRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. EVALUASI SEMUA MODEL"
      ],
      "metadata": {
        "id": "qkHZOPKRtETo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test, is_bert=False):\n",
        "    if is_bert:\n",
        "        y_train = np.argmax(y_train, axis=1)\n",
        "        y_test = n    p.argmax(y_test, axis=1)\n",
        "        train_acc = model.evaluate(X_train, to_categorical(y_train), verbose=0)[1]\n",
        "        test_acc = model.evaluate(X_test, to_categorical(y_test), verbose=0)[1]\n",
        "    else:\n",
        "        train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
        "        test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "    return train_acc, test_acc\n",
        "\n",
        "results = {\n",
        "    'Model': ['LSTM', 'CNN+TF-IDF', 'IndoBERT'],\n",
        "    'Training Accuracy': [\n",
        "        evaluate_model(model_lstm, X_train1, X_test1, y_train1, y_test1)[0],\n",
        "        evaluate_model(model_cnn, X_train2, X_test2, y_train2, y_test2)[0],\n",
        "        evaluate_model(model_bert, X_train3, X_test3, y_train3, y_test3, is_bert=True)[0]\n",
        "    ],\n",
        "    'Testing Accuracy': [\n",
        "        evaluate_model(model_lstm, X_train1, X_test1, y_train1, y_test1)[1],\n",
        "        evaluate_model(model_cnn, X_train2, X_test2, y_train2, y_test2)[1],\n",
        "        evaluate_model(model_bert, X_train3, X_test3, y_train3, y_test3, is_bert=True)[1]\n",
        "    ]\n",
        "}\n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=================================\")\n",
        "print(\"    HASIL AKURASI FINAL\")\n",
        "print(\"=================================\")\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "NXia8A2Dlddo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. INFERENCE CONTOH"
      ],
      "metadata": {
        "id": "6R9MsnQutran"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, is_bert=False):\n",
        "    text_cleaned = preprocess_text(text)\n",
        "    if is_bert:\n",
        "        inputs = bert_tokenizer(text_cleaned, return_tensors='tf', truncation=True, padding=True, max_length=100)\n",
        "        pred = model.predict(dict(inputs)).logits\n",
        "    else:\n",
        "        seq = tokenizer.texts_to_sequences([text_cleaned])\n",
        "        padded = pad_sequences(seq, maxlen=100)\n",
        "        pred = model.predict(padded)\n",
        "    label_idx = np.argmax(pred)\n",
        "    return label_encoder.inverse_transform([label_idx])[0]\n",
        "\n",
        "# Contoh inference\n",
        "sample_texts = [\n",
        "    \"Aplikasinya sangat lambat dan sering error\",\n",
        "    \"Lumayan bagus tapi ada bug minor\",\n",
        "    \"Sangat puas dengan update terbaru!\"\n",
        "]\n",
        "\n",
        "print(\"\\n=================================\")\n",
        "print(\"      CONTOH INFERENCE\")\n",
        "print(\"=================================\")\n",
        "for text in sample_texts:\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    print(f\"LSTM Prediction: {predict_sentiment(text, model_lstm, tokenizer)}\")\n",
        "    print(f\"CNN Prediction: {predict_sentiment(text, model_cnn, tfidf)}\")\n",
        "    print(f\"BERT Prediction: {predict_sentiment(text, model_bert, bert_tokenizer, is_bert=True)}\")"
      ],
      "metadata": {
        "id": "IcEeatictuGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}